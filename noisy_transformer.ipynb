{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9abce638",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9abce638",
        "outputId": "f372bcb3-0573-4190-d321-d515951b6073"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c5d92738510>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "\"\"\"\n",
        "We first implement a GPT1-like language model to generate text. The training data looks like (y0, y1, y2) -> (y1, y2, y3) and we have 3 loss terms. The model will be trained on chunks of data from Hemingway's most well-known novel. The default setting below will produce a model with about 700K parameters. The model should be expected to work better and better as we make the sizes of the parameters (starting with d_model below) bigger and bigger. Collab should allow us to scale to a 10M parameter model without making us to pay.\n",
        "\n",
        "At the end, after training, you'll decode with this model and generate text. Although there are specialized metric for this task (e.g. perplexity), we will simplify the comparisons by just considering training and validation accuracies.\n",
        "\n",
        "The goal of this notebook is to incorporate from various angles noise structures that hypothetically would promise something interesting or surprising.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # can infer the # of independent sequences we will process in parallel from here.\n",
        "block_size = 256 # can infer the maximum context length for predictions from here.\n",
        "max_iters = 5000\n",
        "eval_interval = 500 # answers how often we evaluate across the optimization: every 500 iterations\n",
        "learning_rate = 3e-4 # can set to different values\n",
        "\"\"\"\n",
        "Use 'mps' if on a mac as below:\n",
        "\n",
        "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "\"\"\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# How many batches we use each time we evaluate\n",
        "eval_iters = 200\n",
        "d_model = 96 # could increase this to 386 --> in order to make the model bigger.\n",
        "n_head = 6 # This implied that each head has a dimension for the key, query, and values of d_model / 6.\n",
        "n_layer = 6 # This implies we have 6 turns to mix the embeddigs --- `n_layer` is \"Nx\" in the paper.\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FONrRI-iKuaB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FONrRI-iKuaB",
        "outputId": "b0678b62-a966-4f10-abe7-cae481ab28ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1RlmRmXiWVKpZq98ftdtOIdM2lsA1uw3j\n",
            "To: /content/hemingway.txt\n",
            "\r  0% 0.00/133k [00:00<?, ?B/s]\r100% 133k/133k [00:00<00:00, 75.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 'https://drive.google.com/uc?export=download&id=1RlmRmXiWVKpZq98ftdtOIdM2lsA1uw3j'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1930b1d3",
      "metadata": {
        "id": "1930b1d3"
      },
      "outputs": [],
      "source": [
        "with open('hemingway.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "bade6f32",
      "metadata": {
        "id": "bade6f32"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    return xb, yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "938086a6",
      "metadata": {
        "id": "938086a6"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            logits, loss = model(xb, yb)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "10e26176",
      "metadata": {
        "id": "10e26176"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "BASELINE\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, d_head):\n",
        "        super().__init__()\n",
        "        self.d_head  = d_head\n",
        "        # Map each key, query, or value in to a d_head dimensional model.\n",
        "        self.W_K = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.W_Q = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, T, d_model)\n",
        "        B,T,d = x.shape\n",
        "        k = self.W_K(x)   # (B,T,d_head)\n",
        "        q = self.W_Q(x) # (B,T,head_size)\n",
        "        # compute attention scores (\"affinities\")\n",
        "\n",
        "        # (B T, d) @ (B, d, T) = (B, T, T)\n",
        "        scores = q @ k.transpose(-2,-1) * self.d_head**-0.5 # (B, T, d_head) @ (B, d_head, T) -> (B, T, T)\n",
        "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        a = F.softmax(scores, dim=-1) # (B, T, T)\n",
        "        a = self.dropout(a)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.W_V(x) # (B,T,d)\n",
        "        out = a @ v # (B, T, T) @ (B, T, d) -> (B, T, d)\n",
        "        # These are the values.\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_head):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(d_head) for _ in range(num_heads)])\n",
        "        # This is to project back to the dimension of d_model. In this case, it is just a learned linear map.\n",
        "        self.proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate the different representations per head.\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # Project the concatenation.\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by a non-linearity; this is applied at the token level.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        d_ff = 4 * d_model\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder block: communication followed by computation.\n",
        "    These are stacked on top of each other one after another.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        # Each head gets a smaller dimensional representation of the data.\n",
        "        d_head = d_model // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, d_head)\n",
        "        self.ff = FeedFoward(d_model)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        This is different from te originl transformer paper\n",
        "        In the \"Attention is all you Need\" paper, we had\n",
        "        x = self.ln1(x + self.sa(x))\n",
        "        x = self.ln2(x + self.ffwd(x))\n",
        "        See Figure 1 here, and mimic that: https://arxiv.org/pdf/2002.04745.pdf\n",
        "        \"\"\"\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# class GPT(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         # each token directly reads off the logits for the next token from a lookup table\n",
        "#         self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
        "#         self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
        "#         self.blocks = nn.Sequential(\n",
        "#             *[DecoderBlock(d_model, n_head=n_head) for _ in range(n_layer)]\n",
        "#         )\n",
        "#          # final layer norm\n",
        "#         self.ln = nn.LayerNorm(d_model)\n",
        "#         self.ff = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "#     def forward(self, idx, targets=None):\n",
        "#         B, T = idx.shape\n",
        "\n",
        "#         # idx and targets are both (B,T) tensor of integers\n",
        "#         # (B,T,d_model)\n",
        "#         tok_emb = self.token_embedding_table(idx)\n",
        "#         # (T,d_model)\n",
        "#         pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "#         # Add positional encodings.\n",
        "#         # (B,T,C)\n",
        "#         x = tok_emb + pos_emb\n",
        "\n",
        "#         # Mix up the token representations over and over via the blocks\n",
        "#         # (B,T,C)\n",
        "#         x = self.blocks(x)\n",
        "#         # (B,T,C)\n",
        "#         x = self.ln(x)\n",
        "#         # (B,T,vocab_size)\n",
        "#         logits = self.ff(x)\n",
        "\n",
        "#         if targets is None:\n",
        "#             loss = None\n",
        "#         else:\n",
        "#             B, T, C = logits.shape\n",
        "#             logits = logits.view(B*T, C)\n",
        "#             targets = targets.view(B*T)\n",
        "#             loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "#         return logits, loss\n",
        "\n",
        "#     def generate(self, idx, max_new_tokens):\n",
        "#         \"\"\"\n",
        "#         idx is (B, T) array of indices in the current context.\n",
        "#         This will generate B total paths in parrallel.\n",
        "#         \"\"\"\n",
        "#         self.eval()\n",
        "#         for _ in range(max_new_tokens):\n",
        "#             # crop idx to the last block_size tokens\n",
        "#             # The model only has kowledge of the context of maximum size block_size.\n",
        "#             idx_cond = idx[:, -block_size:]\n",
        "#             # get the predictions\n",
        "#             # (B, T, vocab_size)\n",
        "#             logits, loss = self(idx_cond)\n",
        "#             # focus only on the last time step\n",
        "#             logits = logits[:, -1, :] # becomes (B, vocab_size)\n",
        "#             # apply softmax to get probabilities\n",
        "#             probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
        "#             # sample from the distribution\n",
        "#             idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "#             # append sampled index to the running sequence\n",
        "#             idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "#         self.train()\n",
        "#         return idx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ALTERNATIVE_1: Gaussian Noise Injection\n",
        "\n",
        "- Idea: Inject Gaussian noise directly into the word embeddings or positional embeddings.\n",
        "        This can help the model learn more robust representations by forcing it to handle\n",
        "        slight variations in the input space.\n",
        "\n",
        "- Implementation: After obtaining the word embeddings, add a small amount of Gaussian noise\n",
        "                  (with mean 0 and a chosen variance) before feeding them into the transformer layers.\n",
        "\n",
        "- self.noise_std: A hyperparameter that controls the standard deviation of the Gaussian noise added to the token embeddings.\n",
        "                  A higher value means more noise, which increases the regularization effect.\n",
        "\n",
        "- Noise Addition: During training (self.training), Gaussian noise with mean 0 and standard deviation self.noise_std is added\n",
        "                  to the token embeddings.\n",
        "\n",
        "- Embedding Modification:\n",
        "  - tok_emb: The token embeddings generated from the input indices.\n",
        "  - pos_emb: The position embeddings that are added to the token embeddings.\n",
        "  - Noise Addition: Before passing the combined embeddings through the transformer blocks, Gaussian noise is added to perturb the embeddings slightly.\n",
        "\n",
        "- Training vs. Inference:\n",
        "  - Training: Noise is added only during training. This regularizes the model by forcing it to be robust to small perturbations in the embedding space.\n",
        "  - Inference: During inference (evaluation mode), no noise is added to ensure stable predictions.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, noise_std=0.01):\n",
        "        super().__init__()\n",
        "        # Embedding layers\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[DecoderBlock(d_model, n_head=n_head) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Linear(d_model, vocab_size)\n",
        "        self.noise_std = noise_std  # Standard deviation of the Gaussian noise\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, d_model)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, d_model)\n",
        "        x = tok_emb + pos_emb  # (B, T, d_model)\n",
        "\n",
        "        # Add Gaussian noise to the embeddings\n",
        "        if self.training and self.noise_std > 0:\n",
        "            noise = torch.randn_like(x) * self.noise_std\n",
        "            x = x + noise\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.ff(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        self.train()\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "GlcIntVfSkYN"
      },
      "id": "GlcIntVfSkYN",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "XdEtmrPm7ZCD",
      "metadata": {
        "id": "XdEtmrPm7ZCD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ALTERNATIVE_7: Perturbed Token Mixup\n",
        "- Idea: Combine (or mix) embeddings of different tokens from the same sequence or from different sequences\n",
        "        with some noise. This method is inspired by the mixup technique used in image processing, promoting\n",
        "        smoother transitions between different token representations.\n",
        "\n",
        "-Implementation: For each token in a sequence, mix its embedding with that of a random token from the same or\n",
        "                 a different sequence, adding some noise during the combination.\n",
        "\n",
        "origin of idea: https://medium.com/@lhungting/mixup-a-trivial-but-powerful-image-augmentation-technique-4e2d0725b8e3#:~:text=MixUp%20augmentation%20linearly%20combines%20an,sampled%20from%20a%20Beta%20distribution.\n",
        "\n",
        "\n",
        "\n",
        "- Mixup Probability (mixup_prob): This parameter determines how often the Perturbed Token Mixup is applied during training.\n",
        "                                  A higher probability means more frequent mixups.\n",
        "\n",
        "- Lambda (lam): A random mixing coefficient that determines the ratio between the original and the permuted token embeddings.\n",
        "              The closer lam is to 1, the more the mixed embedding resembles the original one.\n",
        "\n",
        "- Noise Addition (noise_std): This parameter controls the standard deviation of the Gaussian noise added to the mixed embeddings.\n",
        "                            The noise helps the model to handle variations in the input representations.\n",
        "\n",
        "- Mixup Mechanism: If the model is in training mode (self.training) and a random value is less than mixup_prob, the token embeddings are mixed\n",
        "                with those of another random batch. This mixup creates new training examples by combining inputs from different sequences,\n",
        "                effectively augmenting the training data.\n",
        "\n",
        "- Integration with Transformer Blocks: After applying the mixup and noise, the perturbed embeddings are passed through the transformer blocks as usual.\n",
        "\n",
        "\"\"\"\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Embedding layers\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[DecoderBlock(d_model, n_head=n_head) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None, mixup_prob=0.2, noise_std=0.01):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, d_model)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, d_model)\n",
        "        x = tok_emb + pos_emb  # (B, T, d_model)\n",
        "\n",
        "        # Apply Perturbed Token Mixup\n",
        "        if self.training and random.random() < mixup_prob:\n",
        "            # Select a random batch for mixup\n",
        "            perm = torch.randperm(B).to(device)\n",
        "            lam = torch.rand(1).item()  # Lambda for mixing\n",
        "            x = lam * x + (1 - lam) * x[perm]\n",
        "            # Add Gaussian noise for perturbation\n",
        "            x += noise_std * torch.randn_like(x)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.ff(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        self.train()\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# ALTERNATIVE_9: Noisy Residual Connections\n",
        "\n",
        "# - Idea: Add noise to the residual connections that skip layers. This can make the learning process more resilient to errors in the intermediate layers\n",
        "# and encourage the model to learn more robust representations.\n",
        "# - Implementation: Inject Gaussian or uniform noise into the residual connection before adding it back to the layer’s output.\n",
        "\n",
        "# - noise_std: This parameter controls the amount of noise added to the residual connections. You can adjust it based on your experiment.\n",
        "# - torch.randn_like(x): Generates a tensor of the same shape as x with values drawn from a standard normal distribution.\n",
        "# - Adding Noise: Noise is added after the residual connection is computed, making the output slightly perturbed.\n",
        "# This noise is added twice: once after the self-attention block and once after the feed-forward block.\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# class DecoderBlock(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Transformer decoder block: communication followed by computation.\n",
        "#     These are stacked on top of each other one after another.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, d_model, n_head, noise_std=0.1):\n",
        "#         super().__init__()\n",
        "#         d_head = d_model // n_head\n",
        "#         self.sa = MultiHeadAttention(n_head, d_head)\n",
        "#         self.ff = FeedFoward(d_model)\n",
        "#         self.ln1 = nn.LayerNorm(d_model)\n",
        "#         self.ln2 = nn.LayerNorm(d_model)\n",
        "#         self.noise_std = noise_std  # Standard deviation for the Gaussian noise\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Apply self-attention and add Gaussian noise to the residual connection\n",
        "#         residual = x\n",
        "#         x = self.ln1(x)\n",
        "#         x = self.sa(x)\n",
        "#         x = residual + x + torch.randn_like(x) * self.noise_std\n",
        "\n",
        "#         # Apply feed-forward network and add Gaussian noise to the residual connection\n",
        "#         residual = x\n",
        "#         x = self.ln2(x)\n",
        "#         x = self.ff(x)\n",
        "#         x = residual + x + torch.randn_like(x) * self.noise_std\n",
        "\n",
        "#         return x\n"
      ],
      "metadata": {
        "id": "x6GTBGNdHAC_"
      },
      "id": "x6GTBGNdHAC_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5c230527",
      "metadata": {
        "id": "5c230527"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# ALTENATIVE_10: DropConnect in Transformer Weights\n",
        "# - Idea: Instead of dropping out entire neurons or units, randomly drop out individual connections (weights)\n",
        "# within the transformer layers. This is known as DropConnect and can lead to sparser, more efficient learning.\n",
        "# - Implementation: Apply dropout to the weights of the transformer layers (not the activations), randomly setting a fraction of the weights to zero during training.\n",
        "\n",
        "# - DropConnect Function: This function applies DropConnect by randomly setting a portion of the weights to zero based on the drop_prob.\n",
        "\n",
        "# - Modifications in Head and MultiHeadAttention: DropConnect is applied to the weights of the linear layers in the Head and MultiHeadAttention\n",
        "#   classes. This regularizes the attention mechanism by zeroing out random weights.\n",
        "\n",
        "# - Modifications in FeedForward: Similarly, DropConnect is applied to the weights in the feed-forward network, adding regularization to the transformation layers.\n",
        "\n",
        "# - drop_prob: This parameter controls the probability of each weight being set to zero. You can tune this parameter based on your experiment.-\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# def dropconnect(layer, drop_prob):\n",
        "#     \"\"\"Apply DropConnect to the weights of the given layer.\"\"\"\n",
        "#     if not layer.training or drop_prob == 0:\n",
        "#         return layer.weight\n",
        "#     # Create a binary mask with the same shape as the weights\n",
        "#     mask = torch.bernoulli(torch.ones_like(layer.weight) * (1 - drop_prob))\n",
        "#     # Apply the mask to the weights\n",
        "#     return layer.weight * mask\n",
        "\n",
        "# class Head(nn.Module):\n",
        "#     \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "#     def __init__(self, d_head, drop_prob=0.1):\n",
        "#         super().__init__()\n",
        "#         self.d_head = d_head\n",
        "#         self.drop_prob = drop_prob\n",
        "#         self.W_K = nn.Linear(d_model, d_head, bias=False)\n",
        "#         self.W_Q = nn.Linear(d_model, d_head, bias=False)\n",
        "#         self.W_V = nn.Linear(d_model, d_head, bias=False)\n",
        "#         self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, T, d = x.shape\n",
        "#         # Apply DropConnect to the linear layers' weights\n",
        "#         k = F.linear(x, dropconnect(self.W_K, self.drop_prob))   # (B, T, d_head)\n",
        "#         q = F.linear(x, dropconnect(self.W_Q, self.drop_prob))   # (B, T, d_head)\n",
        "#         v = F.linear(x, dropconnect(self.W_V, self.drop_prob))   # (B, T, d_head)\n",
        "\n",
        "#         scores = q @ k.transpose(-2, -1) * self.d_head ** -0.5  # (B, T, T)\n",
        "#         scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "#         a = F.softmax(scores, dim=-1)  # (B, T, T)\n",
        "#         a = self.dropout(a)\n",
        "#         out = a @ v  # (B, T, d_head)\n",
        "#         return out\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "#     def __init__(self, num_heads, d_head, drop_prob=0.1):\n",
        "#         super().__init__()\n",
        "#         self.heads = nn.ModuleList([Head(d_head, drop_prob) for _ in range(num_heads)])\n",
        "#         self.proj = nn.Linear(d_model, d_model)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.drop_prob = drop_prob\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "#         # Apply DropConnect to the projection layer's weights\n",
        "#         out = F.linear(out, dropconnect(self.proj, self.drop_prob))\n",
        "#         out = self.dropout(out)\n",
        "#         return out\n",
        "\n",
        "# class FeedFoward(nn.Module):\n",
        "#     \"\"\"\n",
        "#     A simple linear layer followed by a non-linearity; this is applied at the token level.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, d_model, drop_prob=0.1):\n",
        "#         super().__init__()\n",
        "#         self.drop_prob = drop_prob\n",
        "#         d_ff = 4 * d_model\n",
        "#         self.fc1 = nn.Linear(d_model, d_ff)\n",
        "#         self.fc2 = nn.Linear(d_ff, d_model)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Apply DropConnect to the weights of the linear layers\n",
        "#         x = F.linear(x, dropconnect(self.fc1, self.drop_prob))\n",
        "#         x = F.relu(x)\n",
        "#         x = F.linear(x, dropconnect(self.fc2, self.drop_prob))\n",
        "#         return self.dropout(x)\n",
        "\n",
        "# class DecoderBlock(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Transformer decoder block: communication followed by computation.\n",
        "#     These are stacked on top of each other one after another.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, d_model, n_head, drop_prob=0.1):\n",
        "#         super().__init__()\n",
        "#         d_head = d_model // n_head\n",
        "#         self.sa = MultiHeadAttention(n_head, d_head, drop_prob)\n",
        "#         self.ff = FeedFoward(d_model, drop_prob)\n",
        "#         self.ln1 = nn.LayerNorm(d_model)\n",
        "#         self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.sa(self.ln1(x))\n",
        "#         x = x + self.ff(self.ln2(x))\n",
        "#         return x\n",
        "\n",
        "# class GPT(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
        "#         self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
        "#         self.blocks = nn.Sequential(\n",
        "#             *[DecoderBlock(d_model, n_head=n_head) for _ in range(n_layer)]\n",
        "#         )\n",
        "#         self.ln = nn.LayerNorm(d_model)\n",
        "#         self.ff = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "#     def forward(self, idx, targets=None):\n",
        "#         B, T = idx.shape\n",
        "#         tok_emb = self.token_embedding_table(idx)\n",
        "#         pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "#         x = tok_emb + pos_emb\n",
        "#         x = self.blocks(x)\n",
        "#         x = self.ln(x)\n",
        "#         logits = self.ff(x)\n",
        "\n",
        "#         if targets is None:\n",
        "#             loss = None\n",
        "#         else:\n",
        "#             B, T, C = logits.shape\n",
        "#             logits = logits.view(B * T, C)\n",
        "#             targets = targets.view(B * T)\n",
        "#             loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "#         return logits, loss\n",
        "\n",
        "#     def generate(self, idx, max_new_tokens):\n",
        "#         self.eval()\n",
        "#         for _ in range(max_new_tokens):\n",
        "#             idx_cond = idx[:, -block_size:]\n",
        "#             logits, loss = self(idx_cond)\n",
        "#             logits = logits[:, -1, :]\n",
        "#             probs = F.softmax(logits, dim=-1)\n",
        "#             idx_next = torch.multinomial(probs, num_samples=1)\n",
        "#             idx = torch.cat((idx, idx_next), dim=1)\n",
        "#         self.train()\n",
        "#         return idx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ThiIDDj1gWse",
      "metadata": {
        "id": "ThiIDDj1gWse"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, tolerance=5, min_delta=0):\n",
        "\n",
        "        self.tolerance = tolerance\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, train_loss, validation_loss):\n",
        "        if (validation_loss - train_loss) / train_loss > self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.tolerance:\n",
        "                self.early_stop = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "sJu3FQkBqT_o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "sJu3FQkBqT_o",
        "outputId": "f481abd9-481f-4cde-c05f-2640db88b77c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.706046 M parameters\n",
            "step 0: train loss 4.3288, val loss 4.3201\n",
            "step 500: train loss 2.2168, val loss 2.2290\n",
            "step 1000: train loss 2.0454, val loss 2.0479\n",
            "step 1500: train loss 1.7986, val loss 1.8035\n",
            "step 2000: train loss 1.6482, val loss 1.6666\n",
            "step 2500: train loss 1.5444, val loss 1.5728\n",
            "step 3000: train loss 1.4684, val loss 1.5051\n",
            "step 3500: train loss 1.4042, val loss 1.4594\n",
            "step 4000: train loss 1.3643, val loss 1.4282\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-e772ea80d4ff>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# sample a batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# evaluate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-0d9f80267b2e>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = GPT().to(device)\n",
        "# Print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "early_stopping = EarlyStopping(tolerance=1, min_delta=0.2)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        if iter:\n",
        "          scheduler.step()\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        early_stopping(losses['train'], losses['val'])\n",
        "        if early_stopping.early_stop:\n",
        "          print(\"We stop at epoch {}\".format(iter))\n",
        "          break\n",
        "\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2037f214",
      "metadata": {
        "id": "2037f214"
      },
      "outputs": [],
      "source": [
        "# Start the model with a new line, generate up to 10000 tokens\n",
        "# This is technically doing generations in batches, but here we have a batch size of 1 and 1 element to start in the batch\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
        "open('fake_hemingway.txt', 'w').write(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8163ae4f",
      "metadata": {
        "id": "8163ae4f"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'gpt.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4432ff",
      "metadata": {
        "id": "ea4432ff"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}